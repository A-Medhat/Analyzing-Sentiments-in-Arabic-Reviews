import nltk
import pandas as pd
import torch
# from keras.utils.conv_utils import conv_input_length
from sklearn.feature_extraction.text import TfidfVectorizer
from tashaphyne.stemming import ArabicLightStemmer
import re
import emoji
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")
from sklearn.preprocessing import LabelEncoder

nltk.download('stopwords')

test = pd.read_csv("/kaggle/input/neural/test _no_label.csv")
df = pd.read_csv("/kaggle/input/neural/train1.csv")
# print(df)
df = df.iloc[:]
# print("print data", df)
# print(len(df))
# Remove duplicated
df.review_description.duplicated().sum()
df.drop(df[df.review_description.duplicated() == True].index, axis=0, inplace=True)

# Remove Punctuation
df.review_description = df.review_description.astype(str)
df.review_description = df.review_description.apply(
    lambda x: re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~"""), ' ', x))
df.review_description = df.review_description.apply(lambda x: x.replace('Ø›', "", ))


# print("print data after pun", df.head())


# Define the function to remove consecutive duplicated Arabic words
def remove_duplicate_arabic_words(text):
    # Tokenize the text into words
    words = text.split()

    # Remove consecutive duplicated words
    unique_words = [words[i] for i in range(len(words)) if i == 0 or words[i] != words[i - 1]]

    # Join the unique words back into a sentence
    modified_text = ' '.join(unique_words)

    return modified_text


df['review_description'] = df['review_description'].apply(remove_duplicate_arabic_words)
# Remove StopWords
stopWords = list(set(stopwords.words("arabic")))  ## To remove duplictes and return to list again

# Some words needed to work with to will remove
for word in ['Ù„Ø§', 'Ù„ÙƒÙ†', 'ÙˆÙ„ÙƒÙ†']:
    stopWords.remove(word)
df.review_description = df.review_description.apply(
    lambda x: " ".join([word for word in x.split() if word not in stopWords]))
# print("print data after removing stop words", df.head())

# Replace Emoji by Text
emojis = {"ğŸ™‚": "ÙŠØ¨ØªØ³Ù…", "ğŸ˜‚": "ÙŠØ¶Ø­Ùƒ", "ğŸ’”": "Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†", "ğŸ™‚": "ÙŠØ¨ØªØ³Ù…", "â¤": "Ø­Ø¨", "â¤": "Ø­Ø¨", "ğŸ˜": "Ø­Ø¨", "ğŸ˜­": "ÙŠØ¨ÙƒÙŠ",
          "ğŸ˜¢": "Ø­Ø²Ù†", "ğŸ˜”": "Ø­Ø²Ù†", "â™¥": "Ø­Ø¨", "ğŸ’œ": "Ø­Ø¨", "ğŸ˜…": "ÙŠØ¶Ø­Ùƒ", "ğŸ™": "Ø­Ø²ÙŠÙ†", "ğŸ’•": "Ø­Ø¨", "ğŸ’™": "Ø­Ø¨", "ğŸ˜": "Ø­Ø²ÙŠÙ†",
          "ğŸ˜Š": "Ø³Ø¹Ø§Ø¯Ø©", "ğŸ‘": "ÙŠØµÙÙ‚", "ğŸ‘Œ": "Ø§Ø­Ø³Ù†Øª", "ğŸ˜´": "ÙŠÙ†Ø§Ù…", "ğŸ˜€": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜Œ": "Ø­Ø²ÙŠÙ†", "ğŸŒ¹": "ÙˆØ±Ø¯Ø©", "ğŸ™ˆ": "Ø­Ø¨",
          "ğŸ˜„": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜": "Ù…Ø­Ø§ÙŠØ¯", "âœŒ": "Ù…Ù†ØªØµØ±", "âœ¨": "Ù†Ø¬Ù…Ù‡", "ğŸ¤”": "ØªÙÙƒÙŠØ±", "ğŸ˜": "ÙŠØ³ØªÙ‡Ø²Ø¡", "ğŸ˜’": "ÙŠØ³ØªÙ‡Ø²Ø¡", "ğŸ™„": "Ù…Ù„Ù„",
          "ğŸ˜•": "Ø¹ØµØ¨ÙŠØ©", "ğŸ˜ƒ": "ÙŠØ¶Ø­Ùƒ", "ğŸŒ¸": "ÙˆØ±Ø¯Ø©", "ğŸ˜“": "Ø­Ø²Ù†", "ğŸ’": "Ø­Ø¨", "ğŸ’—": "Ø­Ø¨", "ğŸ˜‘": "Ù…Ù†Ø²Ø¹Ø¬", "ğŸ’­": "ØªÙÙƒÙŠØ±",
          "ğŸ˜": "Ø«Ù‚Ø©", "ğŸ’›": "Ø­Ø¨", "ğŸ˜©": "Ø­Ø²ÙŠÙ†", "ğŸ’ª": "Ø¹Ø¶Ù„Ø§Øª", "ğŸ‘": "Ù…ÙˆØ§ÙÙ‚", "ğŸ™ğŸ»": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ˜³": "Ù…ØµØ¯ÙˆÙ…", "ğŸ‘ğŸ¼": "ØªØµÙÙŠÙ‚",
          "ğŸ¶": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "ğŸŒš": "ØµÙ…Øª", "ğŸ’š": "Ø­Ø¨", "ğŸ™": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ’˜": "Ø­Ø¨", "ğŸƒ": "Ø³Ù„Ø§Ù…", "â˜º": "ÙŠØ¶Ø­Ùƒ", "ğŸ¸": "Ø¶ÙØ¯Ø¹",
          "ğŸ˜¶": "Ù…ØµØ¯ÙˆÙ…", "âœŒ": "Ù…Ø±Ø­", "âœ‹ğŸ»": "ØªÙˆÙ‚Ù", "ğŸ˜‰": "ØºÙ…Ø²Ø©", "ğŸŒ·": "Ø­Ø¨", "ğŸ™ƒ": "Ù…Ø¨ØªØ³Ù…", "ğŸ˜«": "Ø­Ø²ÙŠÙ†", "ğŸ˜¨": "Ù…ØµØ¯ÙˆÙ…",
          "ğŸ¼ ": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "ğŸ": "Ù…Ø±Ø­", "ğŸ‚": "Ù…Ø±Ø­", "ğŸ’Ÿ": "Ø­Ø¨", "ğŸ˜ª": "Ø­Ø²Ù†", "ğŸ˜†": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜£": "Ø§Ø³ØªÙŠØ§Ø¡", "â˜º": "Ø­Ø¨",
          "ğŸ˜±": "ÙƒØ§Ø±Ø«Ø©", "ğŸ˜": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜–": "Ø§Ø³ØªÙŠØ§Ø¡", "ğŸƒğŸ¼": "ÙŠØ¬Ø±ÙŠ", "ğŸ˜¡": "ØºØ¶Ø¨", "ğŸš¶": "ÙŠØ³ÙŠØ±", "ğŸ¤•": "Ù…Ø±Ø¶", "â€¼": "ØªØ¹Ø¬Ø¨",
          "ğŸ•Š": "Ø·Ø§Ø¦Ø±", "ğŸ‘ŒğŸ»": "Ø§Ø­Ø³Ù†Øª", "â£": "Ø­Ø¨", "ğŸ™Š": "Ù…ØµØ¯ÙˆÙ…", "ğŸ’ƒ": "Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­", "ğŸ’ƒğŸ¼": "Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­", "ğŸ˜œ": "Ù…Ø±Ø­",
          "ğŸ‘Š": "Ø¶Ø±Ø¨Ø©", "ğŸ˜Ÿ": "Ø§Ø³ØªÙŠØ§Ø¡", "ğŸ’–": "Ø­Ø¨", "ğŸ˜¥": "Ø­Ø²Ù†", "ğŸ»": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "âœ’": "ÙŠÙƒØªØ¨", "ğŸš¶ğŸ»": "ÙŠØ³ÙŠØ±", "ğŸ’": "Ø§Ù„Ù…Ø§Ø¸",
          "ğŸ˜·": "ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶", "â˜": "ÙˆØ§Ø­Ø¯", "ğŸš¬": "ØªØ¯Ø®ÙŠÙ†", "ğŸ’": "ÙˆØ±Ø¯", "ğŸŒ": "Ø´Ù…Ø³", "ğŸ‘†": "Ø§Ù„Ø§ÙˆÙ„", "âš ": "ØªØ­Ø°ÙŠØ±",
          "ğŸ¤—": "Ø§Ø­ØªÙˆØ§Ø¡", "âœ–": "ØºÙ„Ø·", "ğŸ“": "Ù…ÙƒØ§Ù†", "ğŸ‘¸": "Ù…Ù„ÙƒÙ‡", "ğŸ‘‘": "ØªØ§Ø¬", "âœ”": "ØµØ­", "ğŸ’Œ": "Ù‚Ù„Ø¨", "ğŸ˜²": "Ù…Ù†Ø¯Ù‡Ø´",
          "ğŸ’¦": "Ù…Ø§Ø¡", "ğŸš«": "Ø®Ø·Ø§", "ğŸ‘ğŸ»": "Ø¨Ø±Ø§ÙÙˆ", "ğŸŠ": "ÙŠØ³Ø¨Ø­", "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…", "â­•": "Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡", "ğŸ·": "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
          "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯", "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±", "ğŸŒ": "Ù…Ø¨ØªØ³Ù…", "â¿": "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡", "ğŸ’ªğŸ¼": "Ù‚ÙˆÙŠ", "ğŸ“©": "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
          "â˜•": "Ù‚Ù‡ÙˆÙ‡", "ğŸ˜§": "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©", "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©", "â—": "ØªØ¹Ø¬Ø¨", "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡", "ğŸ‘¯": "Ø§Ø®ÙˆØ§Øª", "Â©": "Ø±Ù…Ø²",
          "ğŸ‘µğŸ½": "Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡", "ğŸ£": "ÙƒØªÙƒÙˆØª", "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹", "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ", "ğŸ‘ğŸ½": "Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡", "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
          "â‰": "Ø§Ø³ØªÙ†ÙƒØ§Ø±", "âš½": "ÙƒÙˆØ±Ù‡", "ğŸ•¶": "Ø­Ø¨", "ğŸˆ": "Ø¨Ø§Ù„ÙˆÙ†", "ğŸ€": "ÙˆØ±Ø¯Ù‡", "ğŸ’µ": "ÙÙ„ÙˆØ³", "ğŸ˜‹": "Ø¬Ø§Ø¦Ø¹", "ğŸ˜›": "ÙŠØºÙŠØ¸",
          "ğŸ˜ ": "ØºØ§Ø¶Ø¨", "âœğŸ»": "ÙŠÙƒØªØ¨", "ğŸŒ¾": "Ø§Ø±Ø²", "ğŸ‘£": "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†", "âŒ": "Ø±ÙØ¶", "ğŸŸ": "Ø·Ø¹Ø§Ù…", "ğŸ‘¬": "ØµØ¯Ø§Ù‚Ø©", "ğŸ°": "Ø§Ø±Ù†Ø¨",
          "â˜‚": "Ù…Ø·Ø±", "âšœ": "Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§", "ğŸ‘": "Ø®Ø±ÙˆÙ", "ğŸ—£": "ØµÙˆØª Ù…Ø±ØªÙØ¹", "ğŸ‘ŒğŸ¼": "Ø§Ø­Ø³Ù†Øª", "â˜˜": "Ù…Ø±Ø­", "ğŸ˜®": "ØµØ¯Ù…Ø©",
          "ğŸ˜¦": "Ù‚Ù„Ù‚", "â­•": "Ø§Ù„Ø­Ù‚", "âœ": "Ù‚Ù„Ù…", "â„¹": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "ğŸ™ğŸ»": "Ø±ÙØ¶", "âšª": "Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡", "ğŸ¤": "Ø­Ø²Ù†", "ğŸ’«": "Ù…Ø±Ø­",
          "ğŸ’": "Ø­Ø¨", "ğŸ”": "Ø·Ø¹Ø§Ù…", "â¤": "Ø­Ø¨", "âœˆ": "Ø³ÙØ±", "ğŸƒğŸ»â€â™€": "ÙŠØ³ÙŠØ±", "ğŸ³": "Ø°ÙƒØ±", "ğŸ¤": "Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡", "ğŸ¾": "ÙƒØ±Ù‡",
          "ğŸ”": "Ø¯Ø¬Ø§Ø¬Ø©", "ğŸ™‹": "Ø³Ø¤Ø§Ù„", "ğŸ“®": "Ø¨Ø­Ø±", "ğŸ’‰": "Ø¯ÙˆØ§Ø¡", "ğŸ™ğŸ¼": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ’‚ğŸ¿ ": "Ø­Ø§Ø±Ø³", "ğŸ¬": "Ø³ÙŠÙ†Ù…Ø§",
          "â™¦": "Ù…Ø±Ø­", "ğŸ’¡": "Ù‚ÙƒØ±Ø©", "â€¼": "ØªØ¹Ø¬Ø¨", "ğŸ‘¼": "Ø·ÙÙ„", "ğŸ”‘": "Ù…ÙØªØ§Ø­", "â™¥": "Ø­Ø¨", "ğŸ•‹": "ÙƒØ¹Ø¨Ø©", "ğŸ“": "Ø¯Ø¬Ø§Ø¬Ø©",
          "ğŸ’©": "Ù…Ø¹ØªØ±Ø¶", "ğŸ‘½": "ÙØ¶Ø§Ø¦ÙŠ", "â˜”": "Ù…Ø·Ø±", "ğŸ·": "Ø¹ØµÙŠØ±", "ğŸŒŸ": "Ù†Ø¬Ù…Ø©", "â˜": "Ø³Ø­Ø¨", "ğŸ‘ƒ": "Ù…Ø¹ØªØ±Ø¶", "ğŸŒº": "Ù…Ø±Ø­",
          "ğŸ”ª": "Ø³ÙƒÙŠÙ†Ø©", "â™¨": "Ø³Ø®ÙˆÙ†ÙŠØ©", "ğŸ‘ŠğŸ¼": "Ø¶Ø±Ø¨", "âœ": "Ù‚Ù„Ù…", "ğŸš¶ğŸ¾â€â™€": "ÙŠØ³ÙŠØ±", "ğŸ‘Š": "Ø¶Ø±Ø¨Ø©", "â—¾": "ÙˆÙ‚Ù", "ğŸ˜š": "Ø­Ø¨",
          "ğŸ”¸": "Ù…Ø±Ø­", "ğŸ‘ğŸ»": "Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ", "ğŸ‘ŠğŸ½": "Ø¶Ø±Ø¨Ø©", "ğŸ˜™": "Ø­Ø¨", "ğŸ¥": "ØªØµÙˆÙŠØ±", "ğŸ‘‰": "Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡", "ğŸ‘ğŸ½": "ÙŠØµÙÙ‚",
          "ğŸ’ªğŸ»": "Ø¹Ø¶Ù„Ø§Øª", "ğŸ´": "Ø§Ø³ÙˆØ¯", "ğŸ”¥": "Ø­Ø±ÙŠÙ‚", "ğŸ˜¬": "Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©", "ğŸ‘ŠğŸ¿": "ÙŠØ¶Ø±Ø¨", "ğŸŒ¿": "ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡", "âœ‹ğŸ¼": "ÙƒÙ Ø§ÙŠØ¯",
          "ğŸ‘": "Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡", "â˜ ": "ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨", "ğŸ‰": "ÙŠÙ‡Ù†Ø¦", "ğŸ”•": "ØµØ§Ù…Øª", "ğŸ˜¿": "ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†", "â˜¹": "ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³", "ğŸ˜˜": "Ø­Ø¨",
          "ğŸ˜°": "Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†", "ğŸŒ¼": "ÙˆØ±Ø¯Ù‡", "ğŸ’‹": "Ø¨ÙˆØ³Ù‡", "ğŸ‘‡": "Ù„Ø§Ø³ÙÙ„", "â£": "Ø­Ø¨", "ğŸ§": "Ø³Ù…Ø§Ø¹Ø§Øª", "ğŸ“": "ÙŠÙƒØªØ¨", "ğŸ˜‡": "Ø¯Ø§ÙŠØ®",
          "ğŸ˜ˆ": "Ø±Ø¹Ø¨", "ğŸƒ": "ÙŠØ¬Ø±ÙŠ", "âœŒğŸ»": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±", "ğŸ”«": "ÙŠØ¶Ø±Ø¨", "â—": "ØªØ¹Ø¬Ø¨", "ğŸ‘": "ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚", "ğŸ”": "Ù‚ÙÙ„",
          "ğŸ‘ˆ": "Ù„Ù„ÙŠÙ…ÙŠÙ†", "â„¢": "Ø±Ù…Ø²", "ğŸš¶ğŸ½": "ÙŠØªÙ…Ø´ÙŠ", "ğŸ˜¯": "Ù…ØªÙØ§Ø¬Ø£", "âœŠ": "ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡", "ğŸ˜»": "Ø§Ø¹Ø¬Ø§Ø¨", "ğŸ™‰": "Ù‚Ø±Ø¯",
          "ğŸ‘§": "Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡", "ğŸ”´": "Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡", "ğŸ’ªğŸ½": "Ù‚ÙˆÙ‡", "ğŸ’¤": "ÙŠÙ†Ø§Ù…", "ğŸ‘€": "ÙŠÙ†Ø¸Ø±", "âœğŸ»": "ÙŠÙƒØªØ¨", "â„": "ØªÙ„Ø¬",
          "ğŸ’€": "Ø±Ø¹Ø¨", "ğŸ˜¤": "ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³", "ğŸ–‹": "Ù‚Ù„Ù…", "ğŸ©": "ÙƒØ§Ø¨", "â˜•": "Ù‚Ù‡ÙˆÙ‡", "ğŸ˜¹": "Ø¶Ø­Ùƒ", "ğŸ’“": "Ø­Ø¨", "â˜„ ": "Ù†Ø§Ø±",
          "ğŸ‘»": "Ø±Ø¹Ø¨", "â": "Ø®Ø·Ø¡", "ğŸ¤®": "Ø­Ø²Ù†", 'ğŸ»': "Ø§Ø­Ù…Ø±"}
emoticons_to_emoji = {":)": "ğŸ™‚", ":(": "ğŸ™", "xD": "ğŸ˜†", ":=(": "ğŸ˜­", ":'(": "ğŸ˜¢", ":'â€‘(": "ğŸ˜¢", "XD": "ğŸ˜‚", ":D": "ğŸ™‚",
                      "â™¬": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "â™¡": "â¤", "â˜»": "ğŸ™‚"}


def checkemojie(text):
    emojistext = []
    for char in text:
        if any(emoji.distinct_emoji_list(char)) and char in emojis.keys():
            emojistext.append(emojis[emoji.distinct_emoji_list(char)[0]])
    return " ".join(emojistext)


def emojiTextTransform(text):
    cleantext = re.sub(r'[^\w\s]', '', text)
    return cleantext + " " + checkemojie(text)


# Apply checkemojie and emojiTextTransform
df['review_description'] = df['review_description'].apply(lambda x: emojiTextTransform(x))
# print("print data after changing the emoji to text", df['review_description'].head())

# Remove Numbers
df.review_description = df.review_description.apply(lambda x: ''.join([word for word in x if not word.isdigit()]))

# Apply Stemming
arabic_stemmer = ArabicLightStemmer()
# Apply stemming to the 'review_description' column
df['review_description'] = df['review_description'].apply(
    lambda x: " ".join([arabic_stemmer.light_stem(word) for word in x.split()]))

# print(len(df))

df.dropna(subset=['review_description'], inplace=True)
review_description = df['review_description']
y = df['rating']  # 1,0,-1
total_words = df['review_description'].dropna().str.split().apply(len).sum()
tokenizer = Tokenizer(num_words=total_words)
tokenizer.fit_on_texts(review_description)
wordindex = tokenizer.word_index
# print(wordindex)
seq = tokenizer.texts_to_sequences(review_description)
seq_pad = pad_sequences(seq, maxlen=75, padding="post", truncating="post")
# print(seq_pad.shape)#(500, 75)


# Assuming the preprocessing up to the padding has been done as in the previous code and stored in `seq_pad` and `y`

# Convert the pad sequences and labels to torch Tensors
X = torch.from_numpy(seq_pad).long()  # Encoded and padded sequences
# y = torch.tensor(y).long()  # Labels
from torch.utils.data import TensorDataset, DataLoader
